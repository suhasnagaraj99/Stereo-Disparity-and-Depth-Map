# -*- coding: utf-8 -*-


import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt
import math
import os

class Project3():
  def __init__(self,name):
    self.name=name
    
  ## Function for getting the fundamental matrix F. It takes the 2 images as the input.
  def Fundamental_Matrix(self,image1,image2):

    ## Creating the feature detector to obtain the featue points from both the images.
    ## SIFT feature detector is chosen for this purpose
    sift_detector = cv.SIFT_create()

    ## Keypoints and descriptors of both the images are extracted using the feature detectors
    keypoints1, descriptors1 = sift_detector.detectAndCompute(image1, None)
    keypoints2, descriptors2 = sift_detector.detectAndCompute(image2, None)

    ## The features detected are matched using a feature matcher. These matches will help us to establish a link between 2 images.
    ## FLANN matcher is chosen for this purpose
    ## The feature detector parameters are initialized
    FLANN_INDEX_KDTREE = 1
    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
    search_params = dict(checks=50)
    flann = cv.FlannBasedMatcher(index_params,search_params)
    ## matches are extracted by employing k-nearest-neighbours based matching using FLANN
    matches = flann.knnMatch(descriptors1,descriptors2,k=2)

    ## 'points1' correspond to the matching points in first image whereas 'points2' correspond to the matching points in second image
    points1 = []
    points2 = []

    ## The matches are filtered based on the relative distance and good matches are considered. From the good matches, 'points1' and corresponding 'points2' are extracted.
    for i,(m,n) in enumerate(matches):
      if m.distance < 0.8*n.distance:
        points2.append(keypoints2[m.trainIdx].pt)
        points1.append(keypoints1[m.queryIdx].pt)

    ## The list of points1 and points2 are converted to numpy array of type float32 as this is the expected format of input to cv.findFundamentalMat function.
    points1 = np.float32(points1)
    points2 = np.float32(points2)

    ## cv.findFundamentalMat function is used to obtain the fundamental matrix. RANSAC based filtering is done to obtain the best/accurate fundamental matrix
    ## The function takes the points1 and points2 extracted from matching the features of the two images, as the input, along with the RANSAC related parameters.
    ## The ransacReprojThreshold is the maximum allowable reprojection error for the point to be considered as inlier. confidence corresponds to the accuracy in estimating the model.
    fundamental_matrix, mask = cv.findFundamentalMat(points1, points2, method=cv.FM_RANSAC, ransacReprojThreshold = 0.5 ,confidence=0.99)

    ## points1 and points2 are then filtered based on a mask generated by the RANSAC
    points1 = points1[mask.ravel()==1]
    points2 = points2[mask.ravel()==1]

    # The fundamental matrix obtained and the matching points are returned
    return fundamental_matrix,points1,points2


  ## Function for getting the essential matrix E. It takes the following inputs: Fundamental matrix obtained in previous step, camera intrinsic matrix corresponding to the 2 images.
  def Essential_matrix(self,Fundamental_matrix,cam1,cam2):
    ## The Essential matrix is calculated from the camera matrices and the computed fundamental matrix
    ## It is given by E = cam2_transpose . Fundamental matrix . cam1
    a = np.dot(np.transpose(cam2),Fundamental_matrix)
    essential_matrix = np.dot(a,cam1)

    ## After getting the essential matrix, it should be normalized (Its determinant is 1 or -1).
    ## For this, singular value decomposition is done on the obtained essential matrix, and the essential matrix is recomputed by carefully choosing the value of diagonal matrix.
    U,_,V = np.linalg.svd(essential_matrix)
    b = np.dot(np.diag([1,1,0]),V)
    essential_matrix = np.dot(U,b)

    ## The recomputed essential matrix is returned
    return essential_matrix

  ## Function for getting the best R and T values. It takes essential matrix and the feature points as the input.
  def get_T_R(self,essential_matrix, points1, points2):
    ## cv.recoverPose function is used to obtain the R and T values. These values correspond to the rotation and translation of the 2nd camera with respect to the first camera.
    ret, R, T, mask = cv.recoverPose(essential_matrix, points1, points2)
    T=T.flatten()
    return R,T

  """# 2. Rectification:

  The rectification of the images is done to ensure horizontal epipolar lines. Rectification allows for 1D search to find corresponding features between 2 images. The rectified images are used to compute disparity and depth maps.
  """

  ## The do_rectification function is used to rectify the images, the feature points and the fundamental matrix. It takes the feature points, the images and the fundamental matrix as the input
  def do_rectification(self,points1,points2,img1, img2, F, cam1, cam2, R, T):

    ## cv.stereoRectify function is used to obtain the homography matrices corresponding to image 1 and image 2
    [R1,R2,P1,P2,Q,roi1,roi2] = cv.stereoRectify(cameraMatrix1=cam1,distCoeffs1=None,cameraMatrix2=cam2,distCoeffs2=None,imageSize=(1920,1080),R=R,T=T)

    ## The first 3 camera matrix of the projection matrix corresponds to the new camera matrix. This is used to find the Homograpgy matrix
    cam1_new = P1[:,:3]
    cam2_new = P2[:,:3]

    ## Homography matrix is given by the formula H = inv(K_new).R.K
    H1=np.dot(cam1_new,np.dot(R1,np.linalg.inv(cam1)))
    H2=np.dot(cam2_new,np.dot(R2,np.linalg.inv(cam2)))

    ## Homography matrices are printed
    print(self.name, ": The Homography Matrix corresponding to Image 1 is: ",'\n',H1,'\n','\n')
    print(self.name, ": The Homography Matrix corresponding to Image 2 is: ",'\n',H2,'\n','\n')

    ## The Images are rectified based on the Homography matrix obtained
    img1_rectified = cv.warpPerspective(img1, H1, (img1.shape[1],img1.shape[0]))
    img2_rectified = cv.warpPerspective(img2, H2, (img1.shape[1],img1.shape[0]))

    ## The feature points are rectified based on the homography matrix
    points1_rectified = cv.perspectiveTransform(points1.reshape(-1, 1, 2), H1).reshape(-1,2)
    points2_rectified = cv.perspectiveTransform(points2.reshape(-1, 1, 2), H2).reshape(-1,2)

    ## The fundamental matrix is rectified based on the computed homography matrix
    F_rectified = np.dot(np.linalg.inv(np.transpose(H2)), np.dot(F, np.linalg.inv(H1)))

    ## Rectification of feature points and fundamental matrix ensures the horizontal epipolar lines

    ## Creating a copy of the rectified images for plotting
    img1_rectified_with_points = img1_rectified.copy()
    img2_rectified_with_points = img2_rectified.copy()

    ## The rectified feature points are plotted on the rectified images
    for i in range(points1_rectified.shape[0]):
        cv.circle(img1_rectified_with_points, (int(points1_rectified[i,0]),int(points1_rectified[i,1])), 5, (0,0,255), -1)
        cv.circle(img2_rectified_with_points, (int(points2_rectified[i,0]),int(points2_rectified[i,1])), 5, (0,0,255), -1)

    ## Plotting the images
    fig, axes = plt.subplots(1, 2, figsize=(20, 10))
    axes[0].imshow(cv.cvtColor(img1_rectified_with_points, cv.COLOR_BGR2RGB))
    axes[0].axis('off')
    axes[0].set_title('Rectified Image 1 with feature points')
    axes[1].imshow(cv.cvtColor(img2_rectified_with_points, cv.COLOR_BGR2RGB))
    axes[1].axis('off')
    axes[1].set_title('Rectified Image 2 with feature points')
    plt.tight_layout()
    plt.savefig(self.name+'_rectified_images.jpg', bbox_inches='tight')
    ## The function returns rectified images, rectified images with feature points pointed on them
    return img1_rectified, img2_rectified, img1_rectified_with_points, img2_rectified_with_points, F_rectified, points1_rectified, points2_rectified

  ## draw_epipolarlines function is used to draw the epipolar lined on the rectified images
  def draw_epilines(self,img1,img2,F,points1,points2):

    ## cv.computeCorrespondEpilines function is used to get the epipolar lines. If the previously computed values are correct, the epilines will be horizontal
    lines1 = cv.computeCorrespondEpilines(points1.reshape(-1,1,2), 1, F)
    lines1 = lines1.reshape(-1, 3)

    ## Drawing the lines on the rectified image
    for line in lines1:
        x0, y0, x1, y1 = map(int, [0, -line[2] / line[1], img2.shape[1], -(line[2] + line[0] * img2.shape[1]) / line[1]])
        cv.line(img2, (x0, y0), (x1, y1), (0, 255, 0), 1)

    ## cv.computeCorrespondEpilines function is used to get the epipolar lines. If the previously computed values are correct, the epilines will be horizontal
    lines2 = cv.computeCorrespondEpilines(points2.reshape(-1,1,2), 2, F)
    lines2 = lines2.reshape(-1, 3)

    ## Drawing the lines on the rectified image
    for line in lines2:
        x0, y0, x1, y1 = map(int, [0, -line[2] / line[1], img1.shape[1], -(line[2] + line[0] * img1.shape[1]) / line[1]])
        cv.line(img1, (x0, y0), (x1, y1), (0, 255, 0), 1)

    ## Plotting and displaying the image after drawing the lines
    fig, axes = plt.subplots(1, 2, figsize=(20, 10))
    axes[0].imshow(cv.cvtColor(img1, cv.COLOR_BGR2RGB))
    axes[0].axis('off')
    axes[0].set_title('Image 1 with epipolar lines')
    axes[1].imshow(cv.cvtColor(img2, cv.COLOR_BGR2RGB))
    axes[1].axis('off')
    axes[1].set_title('Image 2 with epipolar lines')
    plt.tight_layout()
    plt.savefig(self.name+'_epipolar lines.jpg', bbox_inches='tight')
    
  def get_disparity_map(self,img1_rectified, img2_rectified, ndisp, block_size, min_disp,disp12MaxDiff,uniquenessRatio):

    ## The images are first converted to grayscale
    img1_gray = cv.cvtColor(img1_rectified, cv.COLOR_BGR2GRAY)
    img2_gray = cv.cvtColor(img2_rectified, cv.COLOR_BGR2GRAY)

    ## Using cv.StereoSGBM_create function, disparity is calculated using the semi-global block matching (SGBM) algorithm
    ## The parameters of this function are tuned to get best result
    stereo = cv.StereoSGBM_create(minDisparity=min_disp,
                                  numDisparities=ndisp-20,
                                  blockSize=block_size,
                                  P1=8*1*block_size**2,
                                  P2=32*1*block_size**2,
                                  disp12MaxDiff=disp12MaxDiff,
                                  uniquenessRatio=uniquenessRatio,
                                  mode=cv.STEREO_SGBM_MODE_SGBM)

    ## The disparity is computed between the 2 rectified images
    disparity = stereo.compute(img1_gray, img2_gray)

    ## The calculated disparity values are rescaled so that the pixels have a value between 0 and 255
    disparity_rescaled = cv.normalize(disparity, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8U)

    ## Colored version of the disparity map is obrtained using the function cv.applyColorMap
    disparity_color = cv.applyColorMap(disparity_rescaled, cv.COLORMAP_HOT)

    ## Both the grayscale disparity map and the colored disparity map are plotted for visualisation
    fig, axes = plt.subplots(1, 2, figsize=(20, 10))
    axes[0].imshow(disparity_rescaled,'gray')
    axes[0].axis('off')
    axes[0].set_title('Greyscale Disparity Map')
    axes[1].imshow(disparity_rescaled,'hot')
    axes[1].axis('off')
    axes[1].set_title('Color(Heat) Disparity Map')
    plt.tight_layout()
    plt.savefig(self.name+'_disparity.jpg', bbox_inches='tight')
    # saving the disparity maps generated

    cv.imwrite(self.name+'_disparity_gray.jpg',disparity_rescaled)

    cv.imwrite(self.name+'disparity_color.jpg',disparity_color)

    return disparity_rescaled
  
  def get_depth_map(self,disparity_map, baseline, focal_length, depth_limit):

    ## The depth is calculated bu using the formula: Depth = (baseline * focal length) / disparity map
    ## Here the disparity map is added with a small value to prevent division by zero
    depth_map = baseline * focal_length / (disparity_map + 1e-10)

    ## The depth map is then rescaled for better visualization
    depth_map[depth_map > depth_limit] = depth_limit
    depth_map_rescaled = cv.normalize(depth_map, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8U)

    ## A colored depth map is obtained by using the function cv.applyColorMap
    depth_color = cv.applyColorMap(depth_map_rescaled, cv.COLORMAP_HOT)

    ## Both the grayscale and the colored depth map are plotted for visualization
    fig, axes = plt.subplots(1, 2, figsize=(20, 10))
    axes[0].imshow(depth_map_rescaled,'gray')
    axes[0].axis('off')
    axes[0].set_title('Greyscale Depth Map')
    axes[1].imshow(depth_map_rescaled,'hot')
    axes[1].axis('off')
    axes[1].set_title('Color(Heat) Depth Map')
    plt.tight_layout()
    plt.savefig(self.name+'_depth.jpg', bbox_inches='tight')
    # saving the depth maps generated

    cv.imwrite(self.name + '_depth_gray.jpg',depth_map_rescaled)

    cv.imwrite(self.name+'_depth_color.jpg',depth_color)

